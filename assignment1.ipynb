{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# 1.1 Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import as pandas dataframe\n",
    "day_df = pd.read_csv(\"./Bike Sharing Dataset/day.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore data\n",
    "\n",
    "day_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get number of samples (rows), number of features (columns), feature names, and data types\n",
    "\n",
    "print(f\"Number of samples: {day_df.shape[0]}\")\n",
    "print(f\"Number of features: {day_df.shape[1]}\") # NOTE: we don't believe that \"instant\" \n",
    "                                                # counts as a feature, but it is still counted here\n",
    "\n",
    "# feature names\n",
    "print(f\"Feature names: {', '.join(day_df.columns)}\") # NOTE: \"instant\" is still listed here as well\n",
    "\n",
    "# determine datatypes for each feature\n",
    "data_types = [str(x) for x in day_df.dtypes]\n",
    "print(f\"Data types: {', '.join(data_types)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "Regarding data types, the data source further categorizes each variable, revealing the variable types rather than just the data types. This information is summarized below:\n",
    "\n",
    "instant: Integer  \n",
    "dteday: Date  \n",
    "season: Categorical  \n",
    "yr: Categorical  \n",
    "mnth: Categorical  \n",
    "holiday: Binary  \n",
    "weekday: Categorical  \n",
    "workingday: Binary  \n",
    "weathersit: Categorical  \n",
    "temp: Continuous  \n",
    "atemp: Continuous  \n",
    "hum: Continuous  \n",
    "windspeed: Continuous  \n",
    "casual: Integer  \n",
    "registered: Integer  \n",
    "cnt: Integer  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "# 1.2 Data Cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Missing or Malformed Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count NaN entries per column\n",
    "count = day_df.isna().sum()\n",
    "print(f\"NaN entries per column:\\n{count}\")\n",
    "\n",
    "# interestingly, it doesn't seem that there are any NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count ? entries per column\n",
    "count_ques = (day_df == \"?\").sum()\n",
    "print(f\"\\\"?\\\" entries per column:\\n{count_ques}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "Based on this brief analysis, it seems that the data are fairly clean. Visting the data source, it is confirmed that none of the columns have missing variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Irrelevant or Identifier Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing \"instant\" (a sample ID) and \"cnt\" (the target variable)\n",
    "# we also drop dteday, since we already have 'yr', 'mnth' and 'weekday', making 'dteday' redundant. \n",
    "# plus, we can't use a field of type 'object' in our matrix calculations for linear regression\n",
    "# we remove \"casual\" and \"registered\", as their sum gives \"cnt\"\n",
    "\n",
    "day_input = day_df.drop(columns = [\"instant\", \"dteday\", \"cnt\", \"casual\", \"registered\"])\n",
    "\n",
    "# confirm the drop\n",
    "print(f\"New features: {', '.join(day_input.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe for just the output column: \n",
    "day_output = day_df[\"cnt\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## Feature Scaling and Normalization\n",
    "\n",
    "According to the data source, all continuous variables in this dataset have already been normalized. Thus, we will skip this step of the process."
   ]
  },
  {
   "cell_type": "raw",
   "id": "15",
   "metadata": {},
   "source": [
    "## Categorical (Discrete) Features\n",
    "\n",
    "Of the remaining features (as previously discussed), \"season\", \"yr\", \"mnth\", \"hr\", \"holiday\", \"weekday\", \"workingday\", and \"weathersit\" are categorical or binary. The binary variables do not need to be one-hot encoded (because they sort of already are). This is the case for \"holiday\", and \"workingday\". Similarly, \"yr\" only has two categories so does not need to be modified either. \n",
    "\n",
    "While \"season\" and \"weekday\" are cyclical, they are not technically oridinal, as, for example, summer is not \"larger\" than spring or winter. Thus, they will be one-hot encoded. \n",
    "\n",
    "We will skip \"mnth\", as it has a fairly large ranges of values, despite it being cyclical as well. \n",
    "\n",
    "Finally, we will one-hot encode \"weathersit\", as this is clearly categorical, with only a few, non-ordinal categories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace columns with one-hot encoded versions\n",
    "\n",
    "encoded_df = pd.get_dummies(day_input, columns=[\"season\", \"weathersit\", \"weekday\", \"mnth\"],\n",
    "                        dtype=int, drop_first=True)\n",
    "\n",
    "# drop_first=True saves us a column, and is ideal for linear regression\n",
    "# (keeps the columns more independent)\n",
    "\n",
    "print(encoded_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(encoded_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## 1.3 Data Visualization and Exploratory Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "hist_list = [\"cnt\", \"hum\", \"windspeed\", \"temp\"]\n",
    "\n",
    "for x in hist_list:\n",
    "    day_df[x].hist()\n",
    "    plt.xlabel(x)\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title(f\"Histogram of {x}\")\n",
    "    plt.grid(False)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter plots\n",
    "\n",
    "scatter_list = [\"temp\", \"hum\", \"windspeed\"]\n",
    "\n",
    "for s in scatter_list:\n",
    "    day_df.plot.scatter(x=s, y=\"cnt\")\n",
    "    plt.title(f\"Number of Rented Bikes vs. {s}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bar plots\n",
    "\n",
    "bar_list = [\"mnth\", \"yr\", \"season\", \"weekday\", \"weathersit\"]\n",
    "\n",
    "for b in bar_list:\n",
    "    day_df.groupby(b)['cnt'].mean().plot.bar() # average number sold in each category\n",
    "    \n",
    "    plt.title(f\"Average Rented Bikes vs. {b}\")\n",
    "    plt.ylabel(\"Average Count\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "# 2.1 Model Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "To implement the linear regression model, we use the closed-form solution, which seeks to find the weight vector w that minimizes the sum of squared residuals. A crucial first step is the construction of the Design Matrix ($X$), where we append a column of ones to the raw feature set to act as a multiplier for the bias term (intercept), ensuring the model is not forced to pass through the origin. As per the instructions, we avoid matrix inversion to maintain numerical stability. Instead, we use this formula:$$(X^T X)\\mathbf{w} = X^T \\mathbf{y}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression:\n",
    "    def __init__(self):\n",
    "        self.weights = None\n",
    "    \n",
    "    def fit(self, X: np.ndarray, y: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        X: Design matrix (NumPy array)\n",
    "        y: Target vector (NumPy array)\n",
    "        \"\"\"\n",
    "        # 1. Add the bias term (intercept)\n",
    "        # Add column of ones as the first column of X\n",
    "        ones = np.ones((X.shape[0], 1))\n",
    "        X_design = np.append(ones, X, axis=1)\n",
    "        \n",
    "        # 2. Compute the components of the Equation\n",
    "        # X^T * X\n",
    "        XTX = X_design.T @ X_design\n",
    "        # X^T * y\n",
    "        XTy = X_design.T @ y\n",
    "        \n",
    "        # 3. Solve for weights (w)\n",
    "        self.weights = np.linalg.solve(XTX, XTy)\n",
    "    \n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Predicts the target values for the given features.\n",
    "        \"\"\"\n",
    "        # Add the same bias term (ones) for prediction\n",
    "        ones = np.ones((X.shape[0], 1))\n",
    "        X_design = np.append(ones, X, axis=1)\n",
    "        \n",
    "        # y_hat = X * w\n",
    "        return X_design @ self.weights\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "# 2.2 Train/test split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "To ensure the model generalizes well to unseen data, we split the dataset into a training set (80%) and a test set (20%). Since the original data is ordered chronologically by date, we first performed a random shuffle of the data indices. This prevents the model from learning a bias based on time; for example, training only on data from the first year and testing on data from the second year. Shuffling ensures that both the training and test sets contain a representative distribution of seasons, weather conditions, etc. We also set a random seed (21) to ensure that our 'random' shuffle is reproducible, allowing for consistent results across different experimental runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X is our feature set\n",
    "X = encoded_df.values\n",
    "# y is our target: the total count of rented bikes\n",
    "y = day_df['cnt'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Shape of X: {X.shape}\")\n",
    "print(f\"Shape of y: {y.shape}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Set a random seed for reproducibility \n",
    "np.random.seed(21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Generate shuffled indices\n",
    "num_samples = X.shape[0]\n",
    "indices = np.arange(num_samples)\n",
    "np.random.shuffle(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Calculate the number of samples for training data\n",
    "TRAINING_DATA_SPLIT = 0.8\n",
    "training_data_size = int(TRAINING_DATA_SPLIT * num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Split the samples between training/test data\n",
    "train_data_indices = indices[:training_data_size]\n",
    "test_data_indices = indices[training_data_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Create the actual datasets\n",
    "X_train, X_test = X[train_data_indices], X[test_data_indices]\n",
    "y_train, y_test = y[train_data_indices], y[test_data_indices]\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]}\")\n",
    "print(f\"Test set size: {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "# 2.3 Evaluation metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "Here, we implemented the Mean Squared Error (MSE) as our evaluation metric, which was computed separately for both the training and test datasets. Based on our implementation, we achieved a Training MSE of 562,940 and a Test MSE of 620,802. Comparing these two values allows us to assess the model's ability to generalize. Since the error on the unseen test set is within a similar range as the training error, we can conclude that the model effectively captured the underlying patterns in the data rather than simply memorizing the training samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get our linear regression model\n",
    "model = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model on training data set\n",
    "model.fit(X=X_train, y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions on data sets\n",
    "y_hat_train = model.predict(X_train)\n",
    "y_hat_test = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the MSE function\n",
    "def calculate_MSE(y_actual, y_pred):\n",
    "    \"\"\"\n",
    "    Compute the Mean Squared Error between actual and predicted values.\n",
    "    \"\"\"\n",
    "    # Formula: 1/n * sum((y_true - y_pred)^2)\n",
    "    return np.mean((y_actual - y_pred) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the MSE scores\n",
    "train_mse = calculate_MSE(y_actual=y_train, y_pred=y_hat_train)\n",
    "test_mse = calculate_MSE(y_actual=y_test, y_pred=y_hat_test)\n",
    "print(f\"MSE on training set: {train_mse}\")\n",
    "print(f\"MSE on test set: {test_mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "# 3.1 Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our feature for feature engineering\n",
    "fe_df = encoded_df.copy()\n",
    "target = day_df['cnt'].copy()\n",
    "# Continuous variables to use for polynomial features, interaction terms, and simple nonlinear transforms\n",
    "cont_vars = [\"temp\",\"atemp\",\"hum\",\"windspeed\"]\n",
    "\n",
    "for var_i in cont_vars:\n",
    "    #polynomial features (x^2)\n",
    "    fe_df[f\"{var_i}^2\"] = fe_df[var_i]**2\n",
    "    #interaction terms (x_i x_j)\n",
    "    for var_j in cont_vars:\n",
    "        #To avoid duplication of columns (since the value of x_i x_j = x_j x_i)\n",
    "        if (var_i != var_j) and (f\"{var_i} {var_j}\" not in fe_df.columns and f\"{var_j} {var_i}\" not in fe_df.columns):\n",
    "            fe_df[f\"{var_i} {var_j}\"] = fe_df[var_i] * fe_df[var_j]\n",
    "\n",
    "\n",
    "    #simple nonlinear transforms log(x)\n",
    "    #Log transformation iff we have restrictly positive numbers in a feature\n",
    "    if (fe_df[var_i] > 0).all():\n",
    "        fe_df[f\"log({var_i})\"] = np.log(fe_df[var_i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "# 3.2 Model retraining and analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Redoing the LinearRegression on the feature engineered data frame and normalization\n",
    "X_fe = fe_df.values\n",
    "y_fe = day_df['cnt'].values\n",
    "np.random.seed(21)\n",
    "num_samples_fe = X_fe.shape[0]\n",
    "indices_fe = np.arange(num_samples_fe)\n",
    "np.random.shuffle(indices_fe)\n",
    "training_data_size = int(TRAINING_DATA_SPLIT * num_samples_fe)\n",
    "train_data_indices = indices_fe[:training_data_size]\n",
    "test_data_indices = indices_fe[training_data_size:]\n",
    "X_train, X_test = X_fe[train_data_indices], X_fe[test_data_indices]\n",
    "y_train, y_test = y_fe[train_data_indices], y_fe[test_data_indices]\n",
    "#Normalizing our test data set after adding new features\n",
    "scalar = StandardScaler()\n",
    "#Normalizing the training set\n",
    "X_train = scalar.fit_transform(X_train)\n",
    "#Normalizing the test using the value of mean and standard deviation of our training set\n",
    "X_test = scalar.transform(X_test)\n",
    "print(f\"Training set size: {X_train.shape[0]}\")\n",
    "print(f\"Test set size: {X_test.shape[0]}\")\n",
    "model = LinearRegression()\n",
    "model.fit(X=X_train, y=y_train)\n",
    "y_hat_train = model.predict(X_train)\n",
    "y_hat_test = model.predict(X_test)\n",
    "train_mse_fe = calculate_MSE(y_actual=y_train, y_pred=y_hat_train)\n",
    "test_mse_fe = calculate_MSE(y_actual=y_test, y_pred=y_hat_test)\n",
    "print(f\"MSE on training set after feature engineering: {train_mse_fe}\")\n",
    "print(f\"MSE on test set after feature engineering: {test_mse_fe}\")\n",
    "print(f\"MSE on training set: {train_mse}\")\n",
    "print(f\"MSE on test set: {test_mse}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
